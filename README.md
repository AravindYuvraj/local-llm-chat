# local-llm-chat

## How to Use
1. Install [Ollama](https://ollama.com/download) and run: `ollama run llama3`

2. Install Python dependencies:`pip install requests`

3. Run the script: `python chat_with_ollama.py`

This script sends a prompt to a local LLM and prints the AIâ€™s response.



